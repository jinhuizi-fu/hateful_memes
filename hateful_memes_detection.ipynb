{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hateful Memes Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description:\n",
    "\n",
    "### Goal\n",
    "\n",
    "To build a model to predict hateful memes;\n",
    "\n",
    "This is originally a data science competition hosted by facebook.ai;\n",
    "\n",
    "https://www.drivendata.org/competitions/64/hateful-memes/data/\n",
    "\n",
    "### 1. Data Source\n",
    "\n",
    "Data was downloaded from the above website.\n",
    "\n",
    "* The size of the zip file is 3.4GB\n",
    "* Data collection date: 05/12/2020\n",
    "\n",
    "### 2. Method\n",
    "\n",
    "Multimodal deep learning model;\n",
    "\n",
    "Based on this article: https://www.drivendata.co/blog/hateful-memes-benchmark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path  # Path style access for pandas\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Facebook.ai packages\n",
    "import torch #Fuse vision model and language model together                  \n",
    "import torchvision #Vision models and utilities\n",
    "import fasttext #Language models and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd() / \"Desktop\" / \"Hateful_Memes_Detection\" / \"data\"\n",
    "\n",
    "#img_tar_path = data_dir / \"img.tar.gz\"\n",
    "img_tar_path = data_dir / \"img\"\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "dev_path = data_dir / \"dev.jsonl\"\n",
    "test_path = data_dir / \"test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/fujinhuizi/Documents/GitHub/hateful_memes/Desktop/Hateful_Memes_Detection/data/img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cb970f542882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tar_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tar_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1571\u001b[0m                     \u001b[0msaved_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[0;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/fujinhuizi/Documents/GitHub/hateful_memes/Desktop/Hateful_Memes_Detection/data/img'"
     ]
    }
   ],
   "source": [
    "if not (img_tar_path).exists():\n",
    "    with tarfile.open(img_tar_path) as tf:\n",
    "        tf.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  \n",
       "0   its their character not their color that matters  \n",
       "1  don't be afraid to love again everyone is not ...  \n",
       "2                           putting bows on your pet  \n",
       "3  i love everything and everybody! except for sq...  \n",
       "4  everybody loves chocolate chip cookies, even h...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples_frame = pd.read_json(train_path, lines=True)\n",
    "train_samples_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5450\n",
       "1    3050\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples_frame.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_frame.text.map(\n",
    "    lambda text: len(text.split(\" \"))\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "images = [\n",
    "    Image.open(\n",
    "        data_dir / train_samples_frame.loc[i, \"img\"]\n",
    "    ).convert(\"RGB\")\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "for image in images:\n",
    "    print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a callable image_transform with Compose\n",
    "image_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(size=(224, 224)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# convert the images and prepare for visualization.\n",
    "tensor_img = torch.stack(\n",
    "    [image_transform(image) for image in images]\n",
    ")\n",
    "grid = torchvision.utils.make_grid(tensor_img)\n",
    "\n",
    "# plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.axis('off')\n",
    "_ = plt.imshow(grid.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Multimodal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a multimodal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Uses jsonl data to preprocess and serve \n",
    "    dictionary of multimodal tensors for model input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        img_dir,\n",
    "        image_transform,\n",
    "        text_transform,\n",
    "        balance=False,\n",
    "        dev_limit=None,\n",
    "        random_state=0,\n",
    "    ):\n",
    "\n",
    "        self.samples_frame = pd.read_json(\n",
    "            data_path, lines=True\n",
    "        )\n",
    "        self.dev_limit = dev_limit\n",
    "        if balance:\n",
    "            neg = self.samples_frame[\n",
    "                self.samples_frame.label.eq(0)\n",
    "            ]\n",
    "            pos = self.samples_frame[\n",
    "                self.samples_frame.label.eq(1)\n",
    "            ]\n",
    "            self.samples_frame = pd.concat(\n",
    "                [\n",
    "                    neg.sample(\n",
    "                        pos.shape[0], \n",
    "                        random_state=random_state\n",
    "                    ), \n",
    "                    pos\n",
    "                ]\n",
    "            )\n",
    "        if self.dev_limit:\n",
    "            if self.samples_frame.shape[0] > self.dev_limit:\n",
    "                self.samples_frame = self.samples_frame.sample(\n",
    "                    dev_limit, random_state=random_state\n",
    "                )\n",
    "        self.samples_frame = self.samples_frame.reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        self.samples_frame.img = self.samples_frame.apply(\n",
    "            lambda row: (img_dir / row.img), axis=1\n",
    "        )\n",
    "\n",
    "        # https://github.com/drivendataorg/pandas-path\n",
    "        if not self.samples_frame.img.path.exists().all():\n",
    "            raise FileNotFoundError\n",
    "        if not self.samples_frame.img.path.is_file().all():\n",
    "            raise TypeError\n",
    "            \n",
    "        self.image_transform = image_transform\n",
    "        self.text_transform = text_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"This method is called when you do len(instance) \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        return len(self.samples_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This method is called when you do instance[key] \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
    "\n",
    "        image = Image.open(\n",
    "            self.samples_frame.loc[idx, \"img\"]\n",
    "        ).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        text = torch.Tensor(\n",
    "            self.text_transform.get_sentence_vector(\n",
    "                self.samples_frame.loc[idx, \"text\"]\n",
    "            )\n",
    "        ).squeeze()\n",
    "\n",
    "        if \"label\" in self.samples_frame.columns:\n",
    "            label = torch.Tensor(\n",
    "                [self.samples_frame.loc[idx, \"label\"]]\n",
    "            ).long().squeeze()\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text, \n",
    "                \"label\": label\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAndVisionConcat(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        loss_fn,\n",
    "        language_module,\n",
    "        vision_module,\n",
    "        language_feature_dim,\n",
    "        vision_feature_dim,\n",
    "        fusion_output_size,\n",
    "        dropout_p,\n",
    "        \n",
    "    ):\n",
    "        super(LanguageAndVisionConcat, self).__init__()\n",
    "        self.language_module = language_module\n",
    "        self.vision_module = vision_module\n",
    "        self.fusion = torch.nn.Linear(\n",
    "            in_features=(language_feature_dim + vision_feature_dim), \n",
    "            out_features=fusion_output_size\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=fusion_output_size, \n",
    "            out_features=num_classes\n",
    "        )\n",
    "        self.loss_fn = loss_fn\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, text, image, label=None):\n",
    "        text_features = torch.nn.functional.relu(\n",
    "            self.language_module(text)\n",
    "        )\n",
    "        image_features = torch.nn.functional.relu(\n",
    "            self.vision_module(image)\n",
    "        )\n",
    "        combined = torch.cat(\n",
    "            [text_features, image_features], dim=1\n",
    "        )\n",
    "        fused = self.dropout(\n",
    "            torch.nn.functional.relu(\n",
    "            self.fusion(combined)\n",
    "            )\n",
    "        )\n",
    "        logits = self.fc(fused)\n",
    "        pred = torch.nn.functional.softmax(logits)\n",
    "        loss = (\n",
    "            self.loss_fn(pred, label) \n",
    "            if label is not None else label\n",
    "        )\n",
    "        return (pred, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# for the purposes of this post, we'll filter\n",
    "# much of the lovely logging info from our LightningModule\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "class HatefulMemesModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n",
    "            # ok, there's one for-loop but it doesn't count\n",
    "            if data_key not in hparams.keys():\n",
    "                raise KeyError(\n",
    "                    f\"{data_key} is a required hparam in this model\"\n",
    "                )\n",
    "        \n",
    "        super(HatefulMemesModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # assign some hparams that get used in multiple places\n",
    "        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n",
    "        self.language_feature_dim = self.hparams.get(\n",
    "            \"language_feature_dim\", 300\n",
    "        )\n",
    "        self.vision_feature_dim = self.hparams.get(\n",
    "            # balance language and vision features by default\n",
    "            \"vision_feature_dim\", self.language_feature_dim\n",
    "        )\n",
    "        self.output_path = Path(\n",
    "            self.hparams.get(\"output_path\", \"model-outputs\")\n",
    "        )\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # instantiate transforms, datasets\n",
    "        self.text_transform = self._build_text_transform()\n",
    "        self.image_transform = self._build_image_transform()\n",
    "        self.train_dataset = self._build_dataset(\"train_path\")\n",
    "        self.dev_dataset = self._build_dataset(\"dev_path\")\n",
    "        \n",
    "        # set up model and training\n",
    "        self.model = self._build_model()\n",
    "        self.trainer_params = self._get_trainer_params()\n",
    "    \n",
    "    ## Required LightningModule Methods (when validating) ##\n",
    "    \n",
    "    def forward(self, text, image, label=None):\n",
    "        return self.model(text, image, label)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        preds, loss = self.forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        preds, loss = self.eval().forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        \n",
    "        return {\"batch_val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack(\n",
    "            tuple(\n",
    "                output[\"batch_val_loss\"] \n",
    "                for output in outputs\n",
    "            )\n",
    "        ).mean()\n",
    "        \n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizers = [\n",
    "            torch.optim.AdamW(\n",
    "                self.model.parameters(), \n",
    "                lr=self.hparams.get(\"lr\", 0.001)\n",
    "            )\n",
    "        ]\n",
    "        schedulers = [\n",
    "            torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizers[0]\n",
    "            )\n",
    "        ]\n",
    "        return optimizers, schedulers\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset, \n",
    "            shuffle=True, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dev_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "    \n",
    "    ## Convenience Methods ##\n",
    "    \n",
    "    def fit(self):\n",
    "        self._set_seed(self.hparams.get(\"random_state\", 42))\n",
    "        self.trainer = pl.Trainer(**self.trainer_params)\n",
    "        self.trainer.fit(self)\n",
    "        \n",
    "    def _set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _build_text_transform(self):\n",
    "        with tempfile.NamedTemporaryFile() as ft_training_data:\n",
    "            ft_path = Path(ft_training_data.name)\n",
    "            with ft_path.open(\"w\") as ft:\n",
    "                training_data = [\n",
    "                    json.loads(line)[\"text\"] + \"/n\" \n",
    "                    for line in open(\n",
    "                        self.hparams.get(\"train_path\")\n",
    "                    ).read().splitlines()\n",
    "                ]\n",
    "                for line in training_data:\n",
    "                    ft.write(line + \"\\n\")\n",
    "                language_transform = fasttext.train_unsupervised(\n",
    "                    str(ft_path),\n",
    "                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n",
    "                    dim=self.embedding_dim\n",
    "                )\n",
    "        return language_transform\n",
    "    \n",
    "    def _build_image_transform(self):\n",
    "        image_dim = self.hparams.get(\"image_dim\", 224)\n",
    "        image_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(\n",
    "                    size=(image_dim, image_dim)\n",
    "                ),        \n",
    "                torchvision.transforms.ToTensor(),\n",
    "                # all torchvision models expect the same\n",
    "                # normalization mean and std\n",
    "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                torchvision.transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), \n",
    "                    std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return image_transform\n",
    "\n",
    "    def _build_dataset(self, dataset_key):\n",
    "        return HatefulMemesDataset(\n",
    "            data_path=self.hparams.get(dataset_key, dataset_key),\n",
    "            img_dir=self.hparams.get(\"img_dir\"),\n",
    "            image_transform=self.image_transform,\n",
    "            text_transform=self.text_transform,\n",
    "            # limit training samples only\n",
    "            dev_limit=(\n",
    "                self.hparams.get(\"dev_limit\", None) \n",
    "                if \"train\" in str(dataset_key) else None\n",
    "            ),\n",
    "            balance=True if \"train\" in str(dataset_key) else False,\n",
    "        )\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # we're going to pass the outputs of our text\n",
    "        # transform through an additional trainable layer\n",
    "        # rather than fine-tuning the transform\n",
    "        language_module = torch.nn.Linear(\n",
    "                in_features=self.embedding_dim,\n",
    "                out_features=self.language_feature_dim\n",
    "        )\n",
    "        \n",
    "        # easiest way to get features rather than\n",
    "        # classification is to overwrite last layer\n",
    "        # with an identity transformation, we'll reduce\n",
    "        # dimension using a Linear layer, resnet is 2048 out\n",
    "        vision_module = torchvision.models.resnet152(\n",
    "            pretrained=True\n",
    "        )\n",
    "        vision_module.fc = torch.nn.Linear(\n",
    "                in_features=2048,\n",
    "                out_features=self.vision_feature_dim\n",
    "        )\n",
    "\n",
    "        return LanguageAndVisionConcat(\n",
    "            num_classes=self.hparams.get(\"num_classes\", 2),\n",
    "            loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "            language_module=language_module,\n",
    "            vision_module=vision_module,\n",
    "            language_feature_dim=self.language_feature_dim,\n",
    "            vision_feature_dim=self.vision_feature_dim,\n",
    "            fusion_output_size=self.hparams.get(\n",
    "                \"fusion_output_size\", 512\n",
    "            ),\n",
    "            dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n",
    "        )\n",
    "    \n",
    "    def _get_trainer_params(self):\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            filepath=self.output_path,\n",
    "            monitor=self.hparams.get(\n",
    "                \"checkpoint_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            mode=self.hparams.get(\n",
    "                \"checkpoint_monitor_mode\", \"min\"\n",
    "            ),\n",
    "            verbose=self.hparams.get(\"verbose\", True)\n",
    "        )\n",
    "\n",
    "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "            monitor=self.hparams.get(\n",
    "                \"early_stop_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            min_delta=self.hparams.get(\n",
    "                \"early_stop_min_delta\", 0.001\n",
    "            ),\n",
    "            patience=self.hparams.get(\n",
    "                \"early_stop_patience\", 3\n",
    "            ),\n",
    "            verbose=self.hparams.get(\"verbose\", True),\n",
    "        )\n",
    "\n",
    "        trainer_params = {\n",
    "            \"checkpoint_callback\": checkpoint_callback,\n",
    "            \"early_stop_callback\": early_stop_callback,\n",
    "            \"default_save_path\": self.output_path,\n",
    "            \"accumulate_grad_batches\": self.hparams.get(\n",
    "                \"accumulate_grad_batches\", 1\n",
    "            ),\n",
    "            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
    "            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
    "            \"gradient_clip_val\": self.hparams.get(\n",
    "                \"gradient_clip_value\", 1\n",
    "            ),\n",
    "        }\n",
    "        return trainer_params\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def make_submission_frame(self, test_path):\n",
    "        test_dataset = self._build_dataset(test_path)\n",
    "        submission_frame = pd.DataFrame(\n",
    "            index=test_dataset.samples_frame.id,\n",
    "            columns=[\"proba\", \"label\"]\n",
    "        )\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16))\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            preds, _ = self.model.eval().to(\"cpu\")(\n",
    "                batch[\"text\"], batch[\"image\"]\n",
    "            )\n",
    "            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
    "            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n",
    "        submission_frame.proba = submission_frame.proba.astype(float)\n",
    "        submission_frame.label = submission_frame.label.astype(int)\n",
    "        return submission_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \n",
    "    # Required hparams\n",
    "    \"train_path\": train_path,\n",
    "    \"dev_path\": dev_path,\n",
    "    \"img_dir\": data_dir,\n",
    "    \n",
    "    # Optional hparams\n",
    "    \"embedding_dim\": 150,\n",
    "    \"language_feature_dim\": 300,\n",
    "    \"vision_feature_dim\": 300,\n",
    "    \"fusion_output_size\": 256,\n",
    "    \"output_path\": \"model-outputs\",\n",
    "    \"dev_limit\": None,\n",
    "    \"lr\": 0.00005,\n",
    "    \"max_epochs\": 10,\n",
    "    \"n_gpu\": 1,\n",
    "    \"batch_size\": 4,\n",
    "    # allows us to \"simulate\" having larger batches \n",
    "    \"accumulate_grad_batches\": 16,\n",
    "    \"early_stop_patience\": 3,\n",
    "}\n",
    "\n",
    "hateful_memes_model = HatefulMemesModel(hparams=hparams)\n",
    "hateful_memes_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "https://www.drivendata.co/blog/hateful-memes-benchmark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
